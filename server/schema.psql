CREATE TABLE docs (
	id SERIAL PRIMARY KEY,
	url VARCHAR(2048) NOT NULL UNIQUE,
	feed jsonb,
	docscore real,
	lang VARCHAR(2),
	crawl_date TIMESTAMP,
	index_date TIMESTAMP,
	segmented_text text,
	segmented_grams jsonb
);

CREATE TABLE ngrams (
	id SERIAL PRIMARY KEY,
	gram VARCHAR(1024) NOT NULL UNIQUE,
	incidence INTEGER NOT NULL
);

CREATE TABLE docngrams (
	url_id INT4 NOT NULL REFERENCES docs (id),
	gram_id INT4 NOT NULL REFERENCES ngrams (id),
	incidence INTEGER NOT NULL,
	primary key (url_id, gram_id)
);

CREATE INDEX ngrams_id_idx ON ngrams(id);
CREATE INDEX docngrams_gram_incidence_idx ON docngrams(gram_id, incidence ASC);

/*
 * Notes.
 * Use unique id serial as primary key, this has performance gains.
 * deduplication by reference as below. Should help performance and row size when we hit billions/trillions.
 * https://stackoverflow.com/questions/14753155/how-well-is-postgres-handling-duplicates
 */
